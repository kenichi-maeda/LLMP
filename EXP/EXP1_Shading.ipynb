{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e543f9d-97c7-44d5-b2e5-330c6474d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import LLMP as L\n",
    "import torch\n",
    "torch.torch.cuda.empty_cache()\n",
    "\n",
    "query = \"What do you see? You should see a two-dimensional pattern consisting of multiple black lines intersecting to form a series \\\n",
    "         of squares on a white background. The image is 100 by 100 pixels. Estimate the diagonal of the squares in the image (in pixels). \\\n",
    "         Then subtract the value from 100 and tell me the value. It does not have to be precise; please respond with JUST a neumerical range.\\\n",
    "         (Sample Answer: 50-60) \"\n",
    "images = [L.GPImage.figure1('shading') for i in range(10)]\n",
    "models = [\"ChatGPT\", \"LLaVA\", \"CustomLLaVA\"]\n",
    "\n",
    "results = L.Evaluator.run(images, query, models)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46630260-2e51-4050-bd74-c570c287fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatGPT_mse = results[\"ChatGPT\"][\"mse\"]\n",
    "chatGPT_mlae = results[\"ChatGPT\"][\"mlae\"]\n",
    "llava_mse = results[\"LLaVA\"][\"mse\"]\n",
    "llava_mlae = results[\"LLaVA\"][\"mlae\"]\n",
    "custom_llava_mse = results[\"CustomLLaVA\"][\"mse\"]\n",
    "custom_llava_mlae = results[\"CustomLLaVA\"][\"mlae\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d6e46-e44d-44cf-994a-74f9c8d3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ChatGPT mse: \", chatGPT_mse)\n",
    "print(\"ChatGPT mlae:\", chatGPT_mlae)\n",
    "print(\"LLaVA mse:   \", llava_mse)\n",
    "print(\"LLaVA mlae:  \", llava_mlae)\n",
    "print(\"CLLaVA mse:  \", custom_llava_mse)\n",
    "print(\"CLLaVA mlae: \", custom_llava_mlae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc34ec4-d8f8-4e4b-96b5-4238d6104a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc0e59-f7ff-4acf-b45f-af38988224c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7b30ae-d78d-4112-90ee-f711d6712185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-23 07:30:02,020] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "ERROR {'error': {'message': 'Your input image may contain content that is not allowed by our safety system.', 'type': 'invalid_request_error', 'param': None, 'code': 'content_policy_violation'}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m images \u001b[38;5;241m=\u001b[39m [L\u001b[38;5;241m.\u001b[39mGPImage\u001b[38;5;241m.\u001b[39mfigure1(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshading\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m55\u001b[39m)]\n\u001b[1;32m     12\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLaVA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomLLaVA\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mEvaluator\u001b[38;5;241m.\u001b[39mrun(images, query, models)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/LLMP/EXP/../LLMP/evaluator.py:69\u001b[0m, in \u001b[0;36mEvaluator.run\u001b[0;34m(data, query, models)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mranges = re.findall(r'\\b(\\d+)(?:-(\\d+))?\\b', answer)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mif ranges:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    ranges_numbers = [num for range_tuple in ranges for num in range_tuple if num]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    ranges_numbers = [float(r) for r in ranges_numbers]\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<![\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw*.-])\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)?(?:-(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)?))?(?![\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw*.-])\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, answer)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matches:\n\u001b[1;32m     72\u001b[0m     ranges_numbers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMP/lib/python3.11/re/__init__.py:216\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'NoneType'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import LLMP as L\n",
    "import torch\n",
    "torch.torch.cuda.empty_cache()\n",
    "\n",
    "query = \"What do you see? You should see a two-dimensional pattern consisting of multiple black lines intersecting to form a series \\\n",
    "         of squares on a white background. The image is 100 by 100 pixels. Estimate the diagonal of the squares in the image (in pixels). \\\n",
    "         Then subtract the value from 100 and tell me the value. It does not have to be precise; please respond with JUST a neumerical range.\\\n",
    "         (Sample Answer: 50-60) \"\n",
    "images = [L.GPImage.figure1('shading') for i in range(55)]\n",
    "models = [\"ChatGPT\", \"LLaVA\", \"CustomLLaVA\"]\n",
    "\n",
    "results = L.Evaluator.run(images, query, models)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f78397-3ee5-44b3-9aa0-f58761616fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatGPT_mse = results[\"ChatGPT\"][\"mse\"]\n",
    "chatGPT_mlae = results[\"ChatGPT\"][\"mlae\"]\n",
    "chatGPT_mean = results[\"ChatGPT\"][\"mean\"]\n",
    "chatGPT_std = results[\"ChatGPT\"][\"std\"]\n",
    "llava_mse = results[\"LLaVA\"][\"mse\"]\n",
    "llava_mlae = results[\"LLaVA\"][\"mlae\"]\n",
    "llava_mean = results[\"LLaVA\"][\"mean\"]\n",
    "llava_std = results[\"LLaVA\"][\"std\"]\n",
    "custom_llava_mse = results[\"CustomLLaVA\"][\"mse\"]\n",
    "custom_llava_mlae = results[\"CustomLLaVA\"][\"mlae\"]\n",
    "custom_llava_mean = results[\"CustomLLaVA\"][\"mean\"]\n",
    "custom_llava_std = results[\"CustomLLaVA\"][\"std\"]\n",
    "print(\"ChatGPT mse: \", chatGPT_mse)\n",
    "print(\"ChatGPT mlae:\", chatGPT_mlae)\n",
    "print(\"ChatGPT mean:\", chatGPT_mean)\n",
    "print(\"ChatGPT std:\", chatGPT_std)\n",
    "print(\"LLaVA mse:   \", llava_mse)\n",
    "print(\"LLaVA mlae:  \", llava_mlae)\n",
    "print(\"LLaVA mean:  \", llava_mean)\n",
    "print(\"LLaVA std:  \", llava_std)\n",
    "print(\"CLLaVA mse:  \", custom_llava_mse)\n",
    "print(\"CLLaVA mlae: \", custom_llava_mlae)\n",
    "print(\"CLLaVA mean: \", custom_llava_mean)\n",
    "print(\"CLLaVA std: \", custom_llava_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3e4b-98a4-421e-a265-86308c5a97c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
